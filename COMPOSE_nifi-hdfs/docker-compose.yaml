version: '3'
services:
  # hdfs
  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-namenode
    hostname: namenode
    restart: unless-stopped
    ports:
      - 9870:9870
      - 8020:8020
      - 9001:9000 #It is for connecting from Spark
      #- '50070:50070'
    networks:
      big-data-net:
        ipv4_address: 10.0.0.2
    volumes: #Comment this while running the containers for the first time. Remove comment while running for the second, and future times. It adds a physical volume on our local machine, equivalent of running bash commands with -v FOLDER.
      - ${path}/hdfs/namenode:/hadoop/dfs/name
      - ${path}/hdfs/namenode_etc:/etc/hadoop
    env_file:
      - stack.env
    environment:
      CLUSTER_NAME: 'hdfs-namenode'
    cap_add:
      - NET_ADMIN

  hdfs-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-datanode
    hostname: datanode
    restart: unless-stopped
    depends_on:
      - hdfs-namenode
    ports:
      - 9864:9864
    networks: #Add the static address, so the continer will be present in our network.
      big-data-net:
        ipv4_address: 10.0.0.3
    volumes:
      - ${path}/hdfs/datanode:/hadoop/dfs/data
    env_file:
      - stack.env
    environment:
      SERVICE_PRECONDITION: 'namenode:9870' # If it is not present, then the continer won't work.
    cap_add:
      - NET_ADMIN

  hdfs-resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-resourcemanager
    hostname: resourcemanager
    restart: unless-stopped
    depends_on:
      - hdfs-namenode
      - hdfs-datanode
    ports:
      - 8088:8088
      #- 50075:50075
    networks:
      big-data-net:
        ipv4_address: 10.0.0.4
    env_file:
      - stack.env
    environment:
      SERVICE_PRECONDITION: 'namenode:9870 datanode:9864'
    cap_add:
      - NET_ADMIN

  hdfs-nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-nodemanager
    hostname: nodemanager
    restart: unless-stopped
    depends_on:
      - hdfs-namenode
      - hdfs-datanode
      - hdfs-resourcemanager
    ports:
      - 8042:8042
    networks:
      big-data-net:
        ipv4_address: 10.0.0.5
    env_file:
      - stack.env
    environment:
      SERVICE_PRECONDITION: 'namenode:9870 datanode:9864 resourcemanager:8088'
    cap_add:
      - NET_ADMIN

  # nifi
  nifi:
    image: apache/nifi:latest
    container_name: nifi
    hostname: nifi
    restart: unless-stopped
    ports:
      - 8080:8080
    networks:
      big-data-net:
        ipv4_address: 10.0.0.6
    volumes: #Comment this while running the containers for the first time. Remove comment while running for the second, and future times. It adds a physical volume on our local machine, equivalent of running bash commands with -v FOLDER.
      - ${path}/nifi:/opt/nifi/nifi-current/conf
      - ${path}/nifi:/opt/nifi/nifi-current/logs
      - ${path}/hdfs/namenode_etc:/hadoop_namenode_etc
    env_file:
      - stack.env
    environment: #It provides the environmental variables for our service, equivalent of running bash commands with -e VARIABLE.
      - NIFI_WEB_HTTP_PORT=8080
      - news_api_key=${NEWS_IO_API_key}
    cap_add:
      - NET_ADMIN

  # tailscale
  tailscale_nifi:
    image: tailscale/tailscale:stable
    container_name: tailscale_nifi
    hostname: tailscale
    restart: unless-stopped
    privileged: true
    cap_add:
      - NET_ADMIN
      - SYS_ADMIN
    networks:
      big-data-net:
        ipv4_address: 10.0.0.7
    volumes:
      - /dev/net/tun:/dev/net/tun
      - ${path}/tailscale:/var/lib
    env_file:
      - stack.env
    environment:
      - TS_ROUTES=10.0.0.0/16
      - TS_HOSTNAME=nifi-hdfs

  # Python news scraper server
  news-scraper:
    build:
      context: ${path}/scraper
      dockerfile: Dockerfile
    container_name: news-scraper
    hostname: news-scraper
    restart: unless-stopped
    ports:
      - 8012:80
    networks:
      big-data-net:
        ipv4_address: 10.0.0.8
    env_file:
      - stack.env
    cap_add:
      - NET_ADMIN

networks:
  big-data-net:
    name: big-data-net
    driver: bridge
    ipam:
      config:
        - subnet: 10.0.0.0/16

